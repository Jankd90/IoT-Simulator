{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\jgm_6/.cache\\torch\\hub\\OxWearables_ssl-wearables_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 Weights loaded\n",
      "Resnet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv1d(3, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer5): Sequential(\n",
      "      (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Downsample()\n",
      "    )\n",
      "  )\n",
      "  (classifier): EvaClassifier(\n",
      "    (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model from a GitHub repository\n",
    "model = torch.hub.load(\n",
    "    'OxWearables/ssl-wearables',  # Replace with the GitHub username and repository name\n",
    "    'harnet5',          # Replace with the model function name in hubconf.py\n",
    "    class_num=class_num,\n",
    "    pretrained=True        # Set to True if you want to load pretrained weights\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in the model except for those in EvaClassifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classifier layers\n",
    "for param in model.classifier.linear1.parameters():\n",
    "    param.requires_grad = True\n",
    "# for param in model.classifier.linear_new1.parameters():\n",
    "#     param.requires_grad = True\n",
    "# for param in model.classifier.linear_new2.parameters():\n",
    "#     param.requires_grad = True\n",
    "for param in model.classifier.linear2.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaClassifier(\n",
      "  (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 3: If necessary, verify the classifier's output size is now `num_classes`\n",
    "print(model.classifier)  # Check the structure to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state dictionary into the new instance\n",
    "model.classifier.load_state_dict(torch.load(r'D:\\Ali_Thesis\\synthetic_data_generation\\Data\\process_data\\acm_10_meta.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv1d(3, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Downsample()\n",
      "    )\n",
      "    (layer5): Sequential(\n",
      "      (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False, padding_mode=circular)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Downsample()\n",
      "    )\n",
      "  )\n",
      "  (classifier): EvaClassifier(\n",
      "    (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 3: If necessary, verify the classifier's output size is now `num_classes`\n",
    "print(model)  # Check the structure to confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD 5 SECONDS SEQUENCES FROM H5PY DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5(filepath, dataset_name):\n",
    "    with h5py.File(filepath, 'r') as h5file:\n",
    "        data = h5file[dataset_name][:]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(417840, 3, 150)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X30 = read_h5(r'D:\\Ali_Thesis\\synthetic_data_generation\\Data\\Process_canada_data\\P13_5_sec_30hz_acc_sequences.h5', 'data')\n",
    "X30.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5020571 , -0.49900004, -0.50070229, -0.49946417, -0.50041967,\n",
       "        -0.49967205, -0.5002488 , -0.499825  , -0.50010009, -0.49996656,\n",
       "        -0.49995924, -0.50012314, -0.49978011, -0.50034175, -0.49948966,\n",
       "        -0.50077759, -0.49869015, -0.50309197, -0.51390049, -0.49775895,\n",
       "        -0.50122576, -0.49916226, -0.50062818, -0.49950613, -0.50039753,\n",
       "        -0.49967973, -0.50027097, -0.49978087, -0.50016701, -0.49987827,\n",
       "        -0.50006614, -0.5000171 , -0.49982383, -0.50061343, -0.49703134,\n",
       "        -0.48071347, -0.49139242, -0.50159061, -0.49913016, -0.50075367,\n",
       "        -0.49907797, -0.5016764 , -0.48731916, -0.4833831 , -0.48482223,\n",
       "        -0.49909511, -0.50045683, -0.49976811, -0.4999715 , -0.50061386,\n",
       "        -0.49623385, -0.4802237 , -0.49277483, -0.50133907, -0.49936095,\n",
       "        -0.5005003 , -0.4994037 , -0.50114909, -0.48659834, -0.4833576 ,\n",
       "        -0.50003231, -0.4997993 , -0.50021169, -0.49980058, -0.50018553,\n",
       "        -0.49982653, -0.50017627, -0.49983128, -0.50017334, -0.49983185,\n",
       "        -0.50017778, -0.49979072, -0.5002436 , -0.49969096, -0.50044985,\n",
       "        -0.48442451, -0.48532718, -0.50110159, -0.49891236, -0.50327476,\n",
       "        -0.51909949, -0.50869941, -0.49841562, -0.50074412, -0.49954495,\n",
       "        -0.50031668, -0.4997804 , -0.50015814, -0.49989336, -0.50005073,\n",
       "        -0.49999022, -0.49995466, -0.50011143, -0.49980396, -0.50031889,\n",
       "        -0.49947894, -0.50091734, -0.49802584, -0.50877122, -0.51107905,\n",
       "        -0.48471298, -0.49604962, -0.50116369, -0.49933345, -0.50056962,\n",
       "        -0.49933775, -0.50100926, -0.49757158, -0.48371679, -0.48374591,\n",
       "        -0.48860009, -0.50280261, -0.49815237, -0.50154438, -0.49854513,\n",
       "        -0.50150241, -0.49827151, -0.50247033, -0.48483373, -0.49890177,\n",
       "        -0.49995044, -0.5004812 , -0.49921872, -0.50111175, -0.49842498,\n",
       "        -0.5025339 , -0.49292615, -0.48357614, -0.48326675, -0.49305235,\n",
       "        -0.50315284, -0.48829868, -0.48195007, -0.49891445, -0.49963226,\n",
       "        -0.50107682, -0.49760501, -0.48630454, -0.50076435, -0.50031911,\n",
       "        -0.49886426, -0.50250614, -0.49128176, -0.48989276, -0.50463193,\n",
       "        -0.48839637, -0.48240209, -0.49700835, -0.50112963, -0.49919722],\n",
       "       [ 0.1535663 ,  0.15710721,  0.1558129 ,  0.15650434,  0.15609299,\n",
       "         0.15634983,  0.15618545,  0.15629247,  0.15622108,  0.15627122,\n",
       "         0.15623224,  0.15626751,  0.15623018,  0.15627426,  0.1562194 ,\n",
       "         0.15628871,  0.15620142,  0.15631028,  0.15617599,  0.15634006,\n",
       "         0.15614108,  0.15638132,  0.15609161,  0.15644205,  0.1560136 ,\n",
       "         0.15653003,  0.15591242,  0.15666024,  0.1557447 ,  0.15688639,\n",
       "         0.15541806,  0.15741198,  0.15439283,  0.1606296 ,  0.16945557,\n",
       "         0.15360768,  0.1577134 ,  0.15524244,  0.15701539,  0.15563437,\n",
       "         0.15676461,  0.15580725,  0.15664001,  0.1558987 ,  0.15656578,\n",
       "         0.15595428,  0.15653125,  0.15597908,  0.15651433,  0.1559884 ,\n",
       "         0.15651332,  0.15597927,  0.15652235,  0.15596813,  0.15654563,\n",
       "         0.15593471,  0.15659396,  0.15586181,  0.15671386,  0.1556299 ,\n",
       "         0.15736665,  0.17067165,  0.15552725,  0.15656508,  0.15607025,\n",
       "         0.15636641,  0.15616704,  0.15631497,  0.1561939 ,  0.1563031 ,\n",
       "         0.15619569,  0.15630294,  0.15618788,  0.15632084,  0.15617095,\n",
       "         0.15633695,  0.1561553 ,  0.15635241,  0.15613983,  0.15636805,\n",
       "         0.15612388,  0.15638441,  0.15610701,  0.15640188,  0.15608886,\n",
       "         0.15642081,  0.15606906,  0.15644159,  0.15604717,  0.15646471,\n",
       "         0.15602266,  0.15649078,  0.15599483,  0.15652063,  0.15596269,\n",
       "         0.15655541,  0.15592483,  0.15659687,  0.15587911,  0.15664773,\n",
       "         0.15582195,  0.15671279,  0.15574658,  0.15680257,  0.15563128,\n",
       "         0.15694052,  0.15546816,  0.15714887,  0.15519565,  0.15752248,\n",
       "         0.15464475,  0.15843925,  0.15271071,  0.16711714,  0.16391045,\n",
       "         0.15386171,  0.15741308,  0.15568855,  0.15640326,  0.15644581,\n",
       "         0.15568074,  0.15729713,  0.1543656 ,  0.16080387,  0.16910267,\n",
       "         0.1541827 ,  0.15681504,  0.15670484,  0.15421088,  0.16681629,\n",
       "         0.16395795,  0.1528645 ,  0.15855461,  0.15443961,  0.15776967,\n",
       "         0.15490413,  0.15749588,  0.15503172,  0.15756097,  0.15447608,\n",
       "         0.161084  ,  0.17502634,  0.16442153,  0.15400895,  0.1577655 ,\n",
       "         0.1548798 ,  0.15773193,  0.15412718,  0.17095015,  0.15812855],\n",
       "       [ 0.82452475,  0.83038848,  0.82638066,  0.82955158,  0.82692289,\n",
       "         0.8291558 ,  0.82723156,  0.8289044 ,  0.82743877,  0.82872185,\n",
       "         0.82760442,  0.82857857,  0.82773164,  0.8284632 ,  0.82783823,\n",
       "         0.82836297,  0.82793408,  0.82826979,  0.82802622,  0.82817708,\n",
       "         0.82812119,  0.82807791,  0.82822694,  0.82796241,  0.82835685,\n",
       "         0.82780947,  0.82852827,  0.82761365,  0.82877008,  0.82730308,\n",
       "         0.82917246,  0.826754  ,  0.83001242,  0.8252094 ,  0.83460541,\n",
       "         0.83934437,  0.82697031,  0.82661496,  0.83751524,  0.83688388,\n",
       "         0.82503757,  0.82911304,  0.83878991,  0.84634702,  0.84199954,\n",
       "         0.82999069,  0.82754994,  0.84611162,  0.84191964,  0.84536982,\n",
       "         0.84224999,  0.84520854,  0.84230305,  0.84527747,  0.84200333,\n",
       "         0.84629671,  0.83166078,  0.8266362 ,  0.84128922,  0.84525045,\n",
       "         0.84208842,  0.82913545,  0.82694004,  0.82949686,  0.82645116,\n",
       "         0.83030642,  0.82470046,  0.83774455,  0.837193  ,  0.82522111,\n",
       "         0.82983208,  0.82686031,  0.82925254,  0.82676697,  0.84301437,\n",
       "         0.84384399,  0.84332212,  0.82586487,  0.83247609,  0.8415526 ,\n",
       "         0.82510346,  0.83015812,  0.82626504,  0.83024689,  0.82487967,\n",
       "         0.84070055,  0.83369442,  0.8258031 ,  0.82970341,  0.82674993,\n",
       "         0.81513281,  0.84212587,  0.82772212,  0.82818818,  0.82814779,\n",
       "         0.82805561,  0.8282265 ,  0.82794653,  0.8285128 ,  0.82696658,\n",
       "         0.83750567,  0.84740459,  0.82976432,  0.82793313,  0.84144912,\n",
       "         0.83085362,  0.82507769,  0.83321934,  0.83919631,  0.83206412,\n",
       "         0.84117677,  0.82500449,  0.82957316,  0.82767434,  0.8271936 ,\n",
       "         0.83793064,  0.84577162,  0.84318112,  0.83160159,  0.82625129,\n",
       "         0.82931806,  0.82797177,  0.84564081,  0.84274812,  0.84223644,\n",
       "         0.82309402,  0.83647999,  0.84401349,  0.84421904,  0.84317172,\n",
       "         0.84415864,  0.84420672,  0.83175531,  0.83885503,  0.83357607,\n",
       "         0.82368666,  0.84938351,  0.83595634,  0.8356742 ,  0.83621372,\n",
       "         0.83566568,  0.83623634,  0.83551447,  0.83670008,  0.83407125,\n",
       "         0.84706365,  0.84310498,  0.83182813,  0.83955161,  0.83228518]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_acc = X30[:, [1, 2, 3], :]  # Shape becomes (417840, 3, 320)\n",
    "# X_acc\n",
    "X_acc = X30\n",
    "X_acc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE SEQUENCE DID WORK !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for the sequence: 8\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# all_logits = []\n",
    "\n",
    "# # Example: A single sequence from your dataset (replace with your actual data)\n",
    "# # Shape: (8, 320) for one sequence\n",
    "# single_sequence = X_acc[0].astype(np.float32)\n",
    "\n",
    "# # Convert to PyTorch tensor and add a batch dimension\n",
    "# single_sequence_tensor = torch.tensor(single_sequence, dtype=torch.float32).unsqueeze(0)  # Shape: (1, 3, 320)\n",
    "\n",
    "# # Move model and data to the same device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = classifier.to(device)\n",
    "# single_sequence_tensor = single_sequence_tensor.to(device)\n",
    "\n",
    "# # Perform inference\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     output = model(single_sequence_tensor)  # Shape: (1, num_classes)\n",
    "\n",
    "\n",
    "# all_logits.extend(output.cpu().tolist())  # Convert to list and store\n",
    "\n",
    "# # Get predicted class\n",
    "# predicted_class = torch.argmax(output, dim=-1).item()\n",
    "# print(f\"Predicted class for the sequence: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.7843915224075317,\n",
       "  -8.75704574584961,\n",
       "  -9.047948837280273,\n",
       "  0.5426174998283386,\n",
       "  -7.941391468048096,\n",
       "  -4.290830612182617,\n",
       "  -5.96685266494751,\n",
       "  -8.919855117797852,\n",
       "  1.9559062719345093,\n",
       "  -8.95846939086914]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to tensor\n",
    "# all_logits_tensor = torch.tensor(all_logits)\n",
    "\n",
    "# # Find the highest class and logit for each sequence\n",
    "# sequence_highest_classes = torch.argmax(all_logits_tensor, dim=-1).tolist()\n",
    "# sequence_highest_logits = torch.max(all_logits_tensor, dim=-1).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n",
      "[1.9559062719345093]\n"
     ]
    }
   ],
   "source": [
    "# # Output\n",
    "# print(sequence_highest_classes)  # [3, 1, 1]\n",
    "# print(sequence_highest_logits)   # [4.2, 3.4, 2.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE SEQUENCES ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = X_acc.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417840"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification completed using batches.\n",
      "The shape of activities logits: (417840, 10)\n",
      "The shape of activities: (417840,)\n"
     ]
    }
   ],
   "source": [
    "# Define the required parameters\n",
    "all_logits = []\n",
    "results = []\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Process data in batches\n",
    "with torch.no_grad():\n",
    "    for i in range(sequences.shape[0]):\n",
    "        sequence = sequences[i]\n",
    "        \n",
    "        # Convert batch to PyTorch tensor\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Move data and model to the same device\n",
    "        sequence_tensor = sequence_tensor.to(device)\n",
    "        \n",
    "        # Classify the batch\n",
    "        output = model(sequence_tensor)\n",
    "\n",
    "        # Store logits for this batch\n",
    "        all_logits.extend(output.cpu().tolist())  # Convert to list and store\n",
    "\n",
    "        predicted_class = torch.argmax(output, dim=-1).item()\n",
    "\n",
    "        # Append results\n",
    "        results.append(predicted_class)\n",
    "\n",
    "print(\"Classification completed using batches.\")\n",
    "\n",
    "logits_array = np.array(all_logits)\n",
    "results_array = np.array(results)\n",
    "\n",
    "print(f'The shape of activities logits: {logits_array.shape}')\n",
    "print(f'The shape of activities: {results_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-26.08112526, -28.51968956, -26.82648087, -29.02114105,\n",
       "       -25.72355461, -32.41719818, -30.02751541, -28.64265633,\n",
       "       -25.18399811, -28.49422836])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE IN NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('P13_classifications', array=results_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('P13_class_probs', array=logits_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best trial:\n",
    "- Value: 0.7475\n",
    "- Params:\n",
    "  - lstm_layers: 2\n",
    "  - hidden_size: 256\n",
    "  - dropout: 0.26138219898271\n",
    "  - activation: ReLU\n",
    "  - lr: 0.0002829985470448175\n",
    "  - weight_decay: 2.1242103361847734e-05\n",
    "  - batch_size: 64\n",
    "  - optimizer: Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model\n",
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, feature_extractor, lstm_layers=1, hidden_size=256, dropout=0.2, activation='ReLU'):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.feature_extractor = feature_extractor\n",
    "\n",
    "#         # Add a Linear layer to transform (batch_size, seq_length, 1) -> (batch_size, seq_length, 512)\n",
    "#         self.feature_transformer = nn.Linear(1, 512)\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=lstm_layers,\n",
    "#                             batch_first=True, dropout=dropout if lstm_layers > 1 else 0)\n",
    "        \n",
    "#         activation_layer = {\n",
    "#             'ReLU': nn.ReLU(),\n",
    "#             'Tanh': nn.Tanh(),\n",
    "#             'LeakyReLU': nn.LeakyReLU()\n",
    "#         }[activation]\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(hidden_size, 512),\n",
    "#             activation_layer,\n",
    "#             nn.Linear(512, 1024),\n",
    "#             activation_layer,\n",
    "#             nn.Linear(1024, 512),\n",
    "#             activation_layer,\n",
    "#             nn.Linear(512, 10),\n",
    "#             # activation_layer,\n",
    "#             # nn.Linear(256, 206)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         features = self.feature_extractor(x)\n",
    "#         # print(\"Feature extractor output shape:\", features.shape)  # Debugging line to check the shape\n",
    "#         features = self.feature_transformer(features)\n",
    "        \n",
    "#         lstm_out, _ = self.lstm(features)\n",
    "#         lstm_last_output = lstm_out[:, -1, :]\n",
    "#         output = self.classifier(lstm_last_output)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgm_6\\AppData\\Local\\Temp\\ipykernel_18872\\734222708.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(calssifier_path))\n"
     ]
    }
   ],
   "source": [
    "# # Create a new instance of the classifier\n",
    "# classifier = CustomModel(\n",
    "#     feature_extractor=feature_extractor,\n",
    "#     lstm_layers=2,\n",
    "#     hidden_size=256,\n",
    "#     dropout=0.26138219898271,\n",
    "#     activation='ReLU'\n",
    "# )\n",
    "\n",
    "# # Load the state dictionary into the new instance\n",
    "# calssifier_path = r'D:\\Rainbow_DQN_Test\\Test_DQN\\Data\\process_data\\best_model_10_meta_labels.pth'\n",
    "# classifier.load_state_dict(torch.load(calssifier_path))\n",
    "# # classifier.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# # Now `classifier` contains the saved weights\n",
    "# print(\"Model loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
